{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 58,
   "id": "b2e4fa38",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load libs\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "from pandas.plotting import scatter_matrix\n",
    "import matplotlib.pyplot as plt\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.model_selection import cross_val_score\n",
    "from sklearn.model_selection import StratifiedKFold\n",
    "from sklearn.metrics import classification_report\n",
    "from sklearn.metrics import confusion_matrix\n",
    "from sklearn.metrics import accuracy_score\n",
    "from sklearn.linear_model import LinearRegression\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.tree import DecisionTreeClassifier\n",
    "from sklearn.neighbors import KNeighborsClassifier\n",
    "from sklearn.discriminant_analysis import LinearDiscriminantAnalysis\n",
    "from sklearn.naive_bayes import GaussianNB\n",
    "from sklearn.svm import SVC\n",
    "from sklearn.model_selection import KFold\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.model_selection import GridSearchCV\n",
    "from sklearn.ensemble import VotingClassifier\n",
    "from sklearn.ensemble import BaggingClassifier\n",
    "from sklearn.ensemble import AdaBoostClassifier\n",
    "from sklearn.ensemble import GradientBoostingClassifier"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "id": "c64e61c0",
   "metadata": {},
   "outputs": [],
   "source": [
    "# dataset downloading, selecting and reordering columns for further analysis\n",
    "dataset=pd.read_excel(\"cata.xlsx\")\n",
    "dataset=dataset.drop(['name','sibsp', 'parch', 'ticket', 'fare', \n",
    "                      'cabin', 'embarked', 'boat', 'body', 'home.dest'],axis=1)\n",
    "dataset=dataset[['survived','pclass', 'sex', 'age']]\n",
    "\n",
    "# checking the dataset for nan and deleting empty rows\n",
    "dataset.isna().sum()\n",
    "dataset=dataset.dropna(axis=0)\n",
    "\n",
    "# formatting the \"survived\" column into categorical\n",
    "dataset.survived.replace(to_replace=[0,1],value=[\"no\",\"yes\"], inplace=True)\n",
    "dataset.sex.replace(to_replace=[\"female\",\"male\"],value=[0,1], inplace=True)\n",
    "dataset = dataset.astype({'survived':'category','pclass':'int','sex':'int','age':'float'})\n",
    "\n",
    "# split-out validation dataset\n",
    "array = dataset.values\n",
    "X = array[:,1:]\n",
    "y = array[:,0]\n",
    "\n",
    "# set up the train & test split\n",
    "X_train, X_test, Y_train, Y_test = train_test_split(X, y, test_size=0.20, random_state=1)\n",
    "\n",
    "# standardize the dataset manually (except for the categorical values)\n",
    "m=np.mean(X_train[:,2])\n",
    "s=np.std(X_train[:,2])\n",
    "X_train[:,2]=(X_train[:,2]-m)/s\n",
    "X_test[:,2]=(X_test[:,2]-m)/s\n",
    "\n",
    "\n",
    "# the goal is to check different models for prediction (linear and non-linear ones)\n",
    "# individually and then deploy different options of the Ensemble Learning and Decision Trees\n",
    "# the list of the individual models is given below:\n",
    "# Logistic Regression (LR)\n",
    "# Linear Discriminant Analysis (LDA)\n",
    "# K-Nearest Neighbors (KNN)\n",
    "# Gaussian Naive Bayes (NB)\n",
    "# Classification Trees (CART)\n",
    "\n",
    "models = []\n",
    "models.append(('LR', LogisticRegression()))\n",
    "models.append(('LDA', LinearDiscriminantAnalysis()))\n",
    "models.append(('KNN', KNeighborsClassifier()))\n",
    "models.append(('NB', GaussianNB()))\n",
    "models.append(('CART', DecisionTreeClassifier()))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "id": "fac9908b",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "model name - LR, accuracy score - 0.80\n",
      "model name - LDA, accuracy score - 0.80\n",
      "model name - KNN, accuracy score - 0.79\n",
      "model name - NB, accuracy score - 0.80\n",
      "model name - CART, accuracy score - 0.79\n"
     ]
    }
   ],
   "source": [
    "# below is a set of different models for prediction (linear and non-linear ones)\n",
    "# and the result of predictions for the validation (test) dataset\n",
    "# max accuracy score - 0.8 for individual models\n",
    "results = []\n",
    "names = []\n",
    "for name, model in models:\n",
    "    model.fit(X_train, Y_train)\n",
    "    predictions=model.predict(X_test)\n",
    "    print('model name - %.5s, accuracy score - %.2f' % \n",
    "        (name,accuracy_score(Y_test,predictions)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "id": "cd59426d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "VotingClassifier accuracy score -  0.8\n"
     ]
    }
   ],
   "source": [
    "# further I'll experiment with different algorythms of the Ensemble Learning and Decision Trees\n",
    "# to maximize the accuracy of the predictions\n",
    "# 1/ I'm going to start with the Voting Classifier which gives the accuracy score 0.8\n",
    "vc=VotingClassifier(estimators=models)\n",
    "vc.fit(X_train, Y_train)\n",
    "predictions=vc.predict(X_test)\n",
    "vc_as=round(accuracy_score(Y_test,predictions),2)\n",
    "name='VotingClassifier accuracy score - '\n",
    "results.append((name,vc_as))\n",
    "print(name,vc_as)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "id": "2e133b21",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "BaggingClassifier accuracy score -  0.8\n"
     ]
    }
   ],
   "source": [
    "# 2/ I'll use bagging technique - Bootstrap Aggregation to reduce variance of idividual models in the ensemble\n",
    "# n_estimators is set to 300 classification trees\n",
    "# BaggingClassifier also shows the accuracy score 0.8\n",
    "dt=DecisionTreeClassifier()\n",
    "bc=BaggingClassifier(estimator=dt,n_estimators=300,n_jobs=-1)\n",
    "bc.fit(X_train, Y_train)\n",
    "predictions=vc.predict(X_test)\n",
    "bc_as=round(accuracy_score(Y_test,predictions),2)\n",
    "name='BaggingClassifier accuracy score - '\n",
    "results.append((name,bc_as))\n",
    "print(name,bc_as)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "id": "7d9ed84f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "BaggingClassifier accuracy score -  0.8\n",
      "BaggingClassifier OOB accuracy score -  0.78\n"
     ]
    }
   ],
   "source": [
    "# the problem with Bagging is that some instances may not be sampled at all\n",
    "# for this time I'm going to retrieve the Out of Bag (OOB) instances, the accuracy score of OBB is 0.78\n",
    "bc=BaggingClassifier(estimator=dt,n_estimators=300,oob_score=True,n_jobs=-1)\n",
    "bc.fit(X_train, Y_train)\n",
    "predictions=vc.predict(X_test)\n",
    "bc_as=round(accuracy_score(Y_test,predictions),2)\n",
    "oob_as=round(bc.oob_score_,2)\n",
    "print('BaggingClassifier accuracy score - ',bc_as)\n",
    "print('BaggingClassifier OOB accuracy score - ',oob_as)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "id": "1ed8404a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "RandomForestClassifier accuracy score -  0.82\n"
     ]
    }
   ],
   "source": [
    "# 3/ I'll deploy the Random Forests to further randomize the training of individual trees\n",
    "# RandomForestClassifier shows the accuracy score 0.82 \n",
    "rfc=RandomForestClassifier(n_estimators=400,min_samples_leaf=0.17,random_state=1)\n",
    "rfc.fit(X_train, Y_train)\n",
    "predictions=rfc.predict(X_test)\n",
    "rfc_as=round(accuracy_score(Y_test,predictions),2)\n",
    "name='RandomForestClassifier accuracy score - '\n",
    "results.append((name,rfc_as))\n",
    "print(name,rfc_as)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "id": "af071fc5",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 640x480 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# let's analyze further the feature importance for the RandomForestClassifier \n",
    "# surprisingly the class of passengers (pclass) is more important vs sex \n",
    "importances_rfc=pd.Series(rfc.feature_importances_,index=['pclass', 'sex', 'age'])\n",
    "importances_rfc=importances_rfc.sort_values()\n",
    "importances_rfc.plot(kind='barh')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "id": "ce6095c9",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "AdaBoostClassifier accuracy score -  0.78\n"
     ]
    }
   ],
   "source": [
    "# 4/ further I'll deploy boosting - the ensemble method combining several weak learners to form a strong learner\n",
    "# each predictor pays more attention to the instances wrongly predicted by its predecessor\n",
    "# and I start from the Adaptive Boosting (Adaboost) which gives the accuracy score 0.78\n",
    "abc=AdaBoostClassifier(estimator=dt,n_estimators=100)\n",
    "abc.fit(X_train, Y_train)\n",
    "predictions=abc.predict(X_test)\n",
    "abc_as=round(accuracy_score(Y_test,predictions),2)\n",
    "name='AdaBoostClassifier accuracy score - '\n",
    "results.append((name,abc_as))\n",
    "print(name,abc_as)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "id": "a3d4b8b9",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "GradientBoostingClassifier accuracy score -  0.82\n"
     ]
    }
   ],
   "source": [
    "# then I'll exiperiment with the Gradient Boosted Trees which improves the accuracy to 0.82\n",
    "gbc=GradientBoostingClassifier(n_estimators=300,max_depth=1,random_state=1)\n",
    "gbc.fit(X_train, Y_train)\n",
    "predictions=gbc.predict(X_test)\n",
    "gbc_as=round(accuracy_score(Y_test,predictions),2)\n",
    "name='GradientBoostingClassifier accuracy score - '\n",
    "results.append((name,gbc_as))\n",
    "print(name,gbc_as)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "id": "bd1ff97b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Stochastic GradientBoostingClassifier accuracy score -  0.82\n"
     ]
    }
   ],
   "source": [
    "# this time i'll deploy the Stochastic Gradient Boosting method to maximize the accuracy\n",
    "# each tree to sample 80% of the data for training\n",
    "# and each tree to use 20% of available features to perform the best split\n",
    "# SGBC gives the accuracy score 0.82\n",
    "sgbc=GradientBoostingClassifier(n_estimators=300,max_depth=1,random_state=1,subsample=0.8,max_features=0.2)\n",
    "sgbc.fit(X_train, Y_train)\n",
    "predictions=sgbc.predict(X_test)\n",
    "sgbc_as=round(accuracy_score(Y_test,predictions),2)\n",
    "name='Stochastic GradientBoostingClassifier accuracy score - '\n",
    "results.append((name,sgbc_as))\n",
    "print(name,sgbc_as)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "id": "b2a3cd1c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "grid.best_params_:  {'max_depth': 6, 'max_features': 0.8, 'min_samples_leaf': 0.06}\n"
     ]
    }
   ],
   "source": [
    "# and finally to check whether the ensemble learning gave a good accuracy score \n",
    "# vs Hyperparameter Tuning I'll run GridSearchCV for DecisionTreeClassifier to compare the scores\n",
    "param_grid = {'max_depth': [1,2,3,4,5,6], \n",
    "              'min_samples_leaf': [0.04,0.06,0.08,0.1,0.12,0.14,0.16,0.18],\n",
    "              'max_features': [0.2,0.4,0.6,0.8,1]} \n",
    "grid = GridSearchCV(estimator=dt,param_grid=param_grid,scoring='accuracy',cv=10,n_jobs=-1)\n",
    "grid.fit(X_train, Y_train)\n",
    "print('grid.best_params_: ',grid.best_params_)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 70,
   "id": "4e5791bb",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "GridSearchCV DecisionTreeClassifier accuracy score for the train set -  0.78\n"
     ]
    }
   ],
   "source": [
    "grid_as=round(grid.best_score_,2)\n",
    "print('GridSearchCV DecisionTreeClassifier accuracy score for the train set - ',grid_as)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 71,
   "id": "d9392010",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "GridSearchCV DecisionTreeClassifier accuracy score for the test set -  0.81\n"
     ]
    }
   ],
   "source": [
    "# Hyperparameter Tuning for DecisionTreeClassifier gives the accuracy score 0.81\n",
    "best_model=grid.best_estimator_\n",
    "test_acc=round(best_model.score(X_test,Y_test),2)\n",
    "name='GridSearchCV DecisionTreeClassifier accuracy score for the test set - '\n",
    "results.append((name,test_acc))\n",
    "print(name,test_acc)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 72,
   "id": "fda50a0a",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[('VotingClassifier accuracy score - ', 0.8),\n",
       " ('BaggingClassifier accuracy score - ', 0.8),\n",
       " ('RandomForestClassifier accuracy score - ', 0.82),\n",
       " ('AdaBoostClassifier accuracy score - ', 0.78),\n",
       " ('GradientBoostingClassifier accuracy score - ', 0.82),\n",
       " ('Stochastic GradientBoostingClassifier accuracy score - ', 0.82),\n",
       " ('GridSearchCV DecisionTreeClassifier accuracy score for the test set - ',\n",
       "  0.81)]"
      ]
     },
     "execution_count": 72,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# to summarize the results of the experiment:\n",
    "# I deployed different variations of ensemble learning algorythms (VotingClassifer, BaggingClassifier)\n",
    "# also used RandomForestClassifier (for the accuracy score and features selection)\n",
    "# then I utilized different option of boosters: AdaBoostClassifier, GradientBoostingClassifier, Stochastic GradientBoostingClassifier\n",
    "# and finally I launched GridSearchCV for Hyperparameter Tuning for DecisionTreeClassifier to compare the scores\n",
    "# the best accuracy score (0.82): RandomForestClassifier,Stochastic and GradientBoostingClassifier\n",
    "results"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
